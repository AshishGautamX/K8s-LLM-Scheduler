# ğŸš€ AI-Powered Kubernetes Scheduler with Llama-3.3-70B

An intelligent Kubernetes scheduler that uses **Meta's Llama-3.3-70B-Instruct** model via HuggingFace Inference API to make smart pod placement decisions based on real-time cluster metrics.

## âœ¨ Features

- **ğŸ¤– LLM-Powered Scheduling**: Uses Llama-3.3-70B for intelligent node selection
- **ğŸ“¦ Request Caching**: Avoids redundant API calls for similar scheduling decisions
- **ğŸ”„ Retry Logic**: Exponential backoff for failed API requests
- **âš¡ Circuit Breaker**: Prevents cascading failures during API outages
- **ğŸ“Š Metrics & Monitoring**: Comprehensive statistics and performance tracking
- **ğŸ¯ Smart Fallback**: Multiple fallback strategies when LLM is unavailable
- **âš™ï¸ Configurable**: YAML-based configuration for easy customization

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Unscheduled    â”‚
â”‚     Pods        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AI Scheduler  â”‚â”€â”€â”€â”€â”€â–¶â”‚  HuggingFace API â”‚
â”‚  (scheduler.py) â”‚      â”‚  Llama-3.3-70B   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kubernetes     â”‚
â”‚  Nodes          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- **Python 3.8+**
- **Kubernetes cluster** (Minikube, Kind, or production cluster)
- **HuggingFace API token** ([Get one here](https://huggingface.co/settings/tokens))
- **kubectl** configured to access your cluster

## ğŸš€ Quick Start

### 1. Clone and Install

```bash
# Clone the repository
git clone <your-repo-url>
cd AI-Cloud-Scheduling

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
# Copy environment template
cp .env.example .env

# Edit .env and add your HuggingFace token
# HUGGINGFACE_TOKEN=your_token_here
```

### 3. Start Kubernetes Cluster

```bash
# Using Minikube (example with 3 nodes)
minikube start --nodes 3 --driver=docker

# Verify nodes are ready
kubectl get nodes
```

### 4. Run the Scheduler

```bash
# Start the AI scheduler
python scheduler.py
```

You should see:
```
ğŸš€ AI-Powered Kubernetes Scheduler with Llama-3.3-70B
============================================================
Using HuggingFace Inference API
Model: meta-llama/Llama-3.3-70B-Instruct
============================================================
âœ… Initialized HuggingFace client for model: meta-llama/Llama-3.3-70B-Instruct
ğŸ“¡ Endpoint: https://router.huggingface.co
âœ… Connected to Kubernetes cluster
ğŸ‘€ Watching for pods with schedulerName=ai-llama-scheduler
```

### 5. Test with Sample Pods

```bash
# In a new terminal, create test pods
kubectl apply -f ai-test-pods.yaml

# Watch the scheduler make decisions
# Check pod status
kubectl get pods -o wide
```

## âš™ï¸ Configuration

Edit `config.yaml` to customize scheduler behavior:

```yaml
scheduler:
  name: ai-llama-scheduler
  
llm:
  model: meta-llama/Llama-3.3-70B-Instruct
  temperature: 0.3
  max_tokens: 200
  max_retries: 3
  
cache:
  enabled: true
  ttl: 300  # 5 minutes
  
circuit_breaker:
  enabled: true
  failure_threshold: 5
  timeout: 60
```

## ğŸ“ Project Structure

```
AI-Cloud-Scheduling/
â”œâ”€â”€ scheduler.py           # Main scheduler implementation
â”œâ”€â”€ config.yaml           # Configuration file
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env.example         # Environment variables template
â”œâ”€â”€ .gitignore           # Git ignore rules
â”œâ”€â”€ ai-test-pods.yaml    # Sample test pods
â””â”€â”€ README.md            # This file
```

## ğŸ”§ Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `HUGGINGFACE_TOKEN` | HuggingFace API token (required) | - |
| `SCHEDULER_NAME` | Name of the scheduler | `ai-llama-scheduler` |
| `LLM_MODEL` | Model to use | `meta-llama/Llama-3.3-70B-Instruct` |
| `LLM_ENDPOINT` | HuggingFace endpoint | `https://router.huggingface.co` |
| `LOG_LEVEL` | Logging level | `INFO` |
| `LOG_FORMAT` | Log format (text/json) | `text` |

## ğŸ“Š How It Works

1. **Pod Detection**: Scheduler watches for pods with `schedulerName: ai-llama-scheduler`
2. **Context Collection**: Gathers real-time metrics from all cluster nodes
3. **Prompt Construction**: Creates a structured prompt with pod requirements and node states
4. **LLM Decision**: Calls Llama-3.3-70B via HuggingFace API for intelligent node selection
5. **Validation**: Ensures selected node is valid and available
6. **Binding**: Binds the pod to the selected node
7. **Caching**: Caches decision for similar future requests

## ğŸ¯ Fallback Strategies

When the LLM is unavailable, the scheduler uses configurable fallback strategies:

- **`resource_balanced`** (default): Balances CPU, memory, and pod count
- **`least_loaded`**: Selects node with lowest resource usage
- **`round_robin`**: Distributes pods evenly across nodes

## ğŸ“ˆ Monitoring

The scheduler tracks comprehensive metrics:

- Total scheduling requests
- LLM vs fallback decisions
- Cache hit rate
- Average response time
- Circuit breaker trips
- Failed bindings

Access stats in the logs or extend with Prometheus metrics.

## ğŸ› Troubleshooting

### "HUGGINGFACE_TOKEN environment variable is required"
- Get a token from https://huggingface.co/settings/tokens
- Add it to your `.env` file or export it: `export HUGGINGFACE_TOKEN=your_token`

### "Cannot connect to Kubernetes cluster"
- Verify kubectl is configured: `kubectl cluster-info`
- Check kubeconfig: `echo $KUBECONFIG`

### "Circuit breaker is OPEN"
- HuggingFace API is experiencing issues
- Scheduler will use fallback strategy automatically
- Check HuggingFace status: https://status.huggingface.co

### Pods stuck in "Pending"
- Verify scheduler is running
- Check pod has correct `schedulerName: ai-llama-scheduler`
- View scheduler logs for errors

## ğŸ”’ Security Best Practices

- **Never commit `.env` file** to version control
- Store HuggingFace token securely (use secrets management in production)
- Use RBAC to limit scheduler permissions
- Monitor API usage to avoid rate limits

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“š Additional Resources

- [HuggingFace Inference API Docs](https://huggingface.co/docs/api-inference/)
- [Kubernetes Scheduler Documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)
- [Llama-3.3-70B Model Card](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)

## ğŸ™ Acknowledgments

- Meta AI for Llama-3.3-70B model
- HuggingFace for Inference API
- Kubernetes community

---

**Made with â¤ï¸ using AI-powered scheduling**
